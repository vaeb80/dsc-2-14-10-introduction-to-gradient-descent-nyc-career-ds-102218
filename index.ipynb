{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous sections we used Scikit-Learn, Statsmodels and ad-hoc OLS programming to find the best fit line when building a linear regression model. We also mentioned, however, that using closed-form Ordinary Least Squares programming might become computationally expensive when there are many features. Therefore, iterative algorithms like the *gradient descent* algorithm are the basis of many models in statistics and machine learning!\n",
    "\n",
    "You previously saw how after choosing the slope and y-intercept values of a regression line, we can calculate the residual sum of squares (RSS) and related root mean squared error.  We can use either the RSS or RMSE to calculate the accuracy of a line. In this lesson we'll use the RSS to iteratively find the best fit line for our problem at hand!\n",
    "\n",
    "Once calculating the accuracy of a line, we are pretty close to improving upon a line by minimizing the RSS.  This is the task of the gradient descent technique.  But before learning about gradient descent, let's review and ensure that we understand how to evaluate how our line fits our data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be able to:\n",
    "* Understand how to go from RSS to finding a \"best fit\" line\n",
    "* Understand a cost curve and what it displays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of plotting our data and a regression line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, let's imagine that we have an predictor $x$ and a dependent variable $y$. We construct or data in a way that we know that the \"actual\" line that we're looking at has an \n",
    "- intercept, $\\beta_0$ of 3 (which we will refer to as $b$ when estimating it)\n",
    "- $\\beta_1$ of 50 (which we will refer to as $m$ when estimating it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(225)\n",
    "\n",
    "x = np.random.rand(30, 1).reshape(30)\n",
    "y_randterm = np.random.normal(0,3,30)\n",
    "y = 3+ 50* x + y_randterm\n",
    "\n",
    "plt.plot(x, y, '.b')\n",
    "plt.xlabel(\"x\", fontsize=14)\n",
    "plt.ylabel(\"y\", fontsize=14);\n",
    "\n",
    "#data = np.array([x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_equations import build_regression_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'slope': 54.550106055369, 'int': 1.3188915249780635}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_regression_line(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again come up with some numbers for a slope and a y-intercept.  \n",
    "\n",
    ">One (not so great) technique to get the slope, is by drawing a line between the first and last points.  And from there, we calculate the value of $b$.  You can use the `build_regression_line` function, defined in our linear_equations library, `linear_equatuions.py`, which quickly does this for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass $x$ and $y$ into our `build_regression_line` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning this into a regression formula, we have the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_formula(x):\n",
    "    return 1.319 + 54.550*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this regression formula with our data to get a sense of what it looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(225)\n",
    "\n",
    "x = np.random.rand(30,1).reshape(30)\n",
    "y_randterm = np.random.normal(0,3,30)\n",
    "y = 3+ 50* x + y_randterm\n",
    "\n",
    "plt.plot(x, y, '.b')\n",
    "plt.plot(x, regression_formula(x), '-')\n",
    "plt.xlabel(\"x\", fontsize=14)\n",
    "plt.ylabel(\"y\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the regression line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, we can calculate the `residual sum of squared errors` and the `root mean squared error`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def errors(x_values, y_values, m, b):\n",
    "    y_line = (b + m*x_values)\n",
    "    return (y_values - y_line)\n",
    "\n",
    "def squared_errors(x_values, y_values, m, b):\n",
    "    return np.round(errors(x_values, y_values, m, b)**2, 2)\n",
    "\n",
    "def residual_sum_squares(x_values, y_values, m, b):\n",
    "    return round(sum(squared_errors(x_values, y_values, m, b)), 2)\n",
    "\n",
    "def root_mean_squared_error(x_values, y_values, m, b):\n",
    "    return round(math.sqrt(sum(squared_errors(x_values, y_values, m, b)))/len(x_values), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.44,  0.84, 20.68, 14.68,  2.43, 13.23, 10.31, 10.61,  1.2 ,\n",
       "        2.04, 38.08,  5.67, 15.21,  0.43,  2.41, 23.48, 37.39,  4.93,\n",
       "        0.38,  0.7 ,  0.  ,  0.  ,  5.65,  6.94, 40.66,  0.23,  0.89,\n",
       "        0.  , 11.44,  5.04])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_errors(x, y, 54.550, 1.319 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279.99"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residual_sum_squares(x, y, 54.550, 1.319) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_mean_squared_error(x, y, 54.550, 1.319) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the RSS is equal to 279.99 and the RMSE is 0.56 for our regression line! Do you think we can do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving towards gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the residual sum of squares function to evaluate the accuracy of our regression line, we can simply try out different regression lines and use the regression line that has the lowest RSS.  The regression line that produces the lowest RSS for a given dataset is called the \"best fit\" line for that dataset.  \n",
    "\n",
    "So this will be our technique for finding our \"best fit\" line:\n",
    "\n",
    "> * Choose a regression line with a guess of values for $m$ and $b$\n",
    "> * Calculate the RSS\n",
    "> * Adjust $m$ and $b$, as these are the only things that can vary in a single-variable regression line.\n",
    "> * Again calculate the RSS \n",
    "> * Repeat this process\n",
    "> * The regression line (that is, the values of $b$ and $m$) with the smallest RSS is our **best fit line**\n",
    "\n",
    "We'll eventually tweak and improve upon that process, but for now it will do.  In fact, we will make things even easier at first by holding $b$ fixed to a constant value while we experiment with different $m$ values.  In later lessons, we will change both variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating the regression line to improve accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we have a regression line of $\\overline{y} = \\overline{m}x + \\overline{b} $, and we started with values of $b = 1.319 $ and $m = 54.550 $.  Then seeing how well this regression line matched our dataset, we calculated that $ RSS = 279.99 $.  Our next step is to plug in different values of $b$ and see how RSS changes.  Let's try $m$ = 54 instead of $54.550$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257.35"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residual_sum_squares(x, y, 54, 1.319)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the RSS is already lower here! Now let's the RSS for a variety of $m$ values. We'll look at $m$-values between 40 and 59."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = np.zeros((20,2))\n",
    "for idx, val in enumerate(range(40, 60)):\n",
    "    table[idx,0] = val\n",
    "    table[idx,1] = residual_sum_squares(x, y, val, 1.319)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  40.  , 2012.31],\n",
       "       [  41.  , 1738.24],\n",
       "       [  42.  , 1487.03],\n",
       "       [  43.  , 1258.71],\n",
       "       [  44.  , 1053.31],\n",
       "       [  45.  ,  870.75],\n",
       "       [  46.  ,  711.05],\n",
       "       [  47.  ,  574.3 ],\n",
       "       [  48.  ,  460.37],\n",
       "       [  49.  ,  369.35],\n",
       "       [  50.  ,  301.17],\n",
       "       [  51.  ,  255.89],\n",
       "       [  52.  ,  233.49],\n",
       "       [  53.  ,  234.  ],\n",
       "       [  54.  ,  257.35],\n",
       "       [  55.  ,  303.64],\n",
       "       [  56.  ,  372.74],\n",
       "       [  57.  ,  464.75],\n",
       "       [  58.  ,  579.66],\n",
       "       [  59.  ,  717.43]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice what the above table represents.  While keeping our value of $b$ fixed at 1.319, we moved towards a smaller residual sum of squares (RSS) by changing our value of $m$, our slope. \n",
    "\n",
    "Setting $m$ to 54 produced a lower error than at 54.550.  We kept moving our $b$ value lower until we set $m$ = 52, at which point our error began to increase.  Therefore, we know that a value of $b$ between 52 and 53 produces the smallest RSS for our data while $b = 1.319$. \n",
    "\n",
    "This changing output of RSS based on a changing input of different regression lines is called our **cost function**.  Let's plot this chart to see it better.\n",
    "\n",
    "We set:\n",
    "\n",
    "* `m_values` as the input values (x values), and\n",
    "* `rss_errors` as the output values (y values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(table[:,0], table[:,1], '-')\n",
    "plt.xlabel(\"m-values\", fontsize=14)\n",
    "plt.ylabel(\"RSS\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above is called the **cost curve**.  It is a plot of the RSS for different values of $m$.    The curve demonstrates that when $m$ is between 52 and 53, the RSS is lowest.  This technique of optimizing towards a minimum value is called *gradient descent*.  Here, we *descend* along a cost curve.  As we change our variable, we need to stop when the value of our RSS no longer decreases.\n",
    "\n",
    "Note that we end up with a m-value which is not quite the same as $m$ the value which generated the data. The reason for this is 2-fold:\n",
    "- We used a value for $b$ which was equal to 1.319, which was quite far from the actual value of 3. Therefore, there is a measurement in error for the slope.\n",
    "- Secondly, randomness was (intentionally) included in our data, so even with the best algorithm, we would come up with a value *close* to 50, but not exactly 50!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section we saw the path from going from calculating the RSS for a given regression line, to finding a line that minimizes our RSS - a best fit line.  We learned that we can move to a better regression line by descending along our cost curve.  Going forward, we will learn how to move towards our best fit line in an efficient manner. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
